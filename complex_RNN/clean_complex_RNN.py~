import cPickle
import gzip
import theano
import numpy as np
import theano.tensor as T

def initialize_matrix(n_in, n_out, name, rng):
    bin = np.sqrt(6. / (n_in + n_out))
    values = np.asarray(rng.uniform(low=-bin,
                                    high=bin,
                                    size=(n_in, n_out)),
                        dtype=theano.config.floatX)
    return theano.shared(value=values, name=name)


# computes Theano graph
# returns symbolic parameters, costs, inputs 
# there are n_hidden real units and a further n_hidden imaginary units 
def complex_RNN(n_input, n_hidden, n_output, time_steps):
    
    np.random.seed(1234)
    #srng = T.shared_randomstreams.RandomStreams(seed=234)
    rng = np.random.RandomState(1234)

    # Initialize parameters: theta, V_re, V_im, hidden_bias, U, out_bias, h_0
    V_re = initialize_matrix(n_input, n_hidden, 'V_re', rng)
    V_im = initialize_matrix(n_input, n_hidden, 'V_im', rng)
    U = initialize_matrix(2 * n_hidden, n_output, 'U', rng)
    hidden_bias = theano.shared(np.zeros((n_hidden,), dtype=theano.config.floatX), name='hidden_bias')
    out_bias = theano.shared(np.zeros((n_output,), dtype=theano.config.floatX), name='out_bias')
    theta = initialize_matrix(3, n_hidden, 'theta', rng)
    h_0 = theano.shared(np.asarray(rng.uniform(low=-0.1,
                                               high=0.1,
                                               size=(1, 2 * n_hidden)), 
                                   dtype=theano.config.floatX),
                        name='h_0')
#    h_0 = T.addbroadcast(h_0, 0)
#    import pdb; pdb.set_trace()

    parameters = [V_re, V_im, hidden_bias, U, out_bias, theta, h_0]

    x = T.tensor3()
    y = T.tensor3()
#    x.tag.test_value = np.random.rand(100,10,1).astype('float32') 
#    y.tag.test_value = np.random.rand(100,10,1).astype('float32')
     


    # TEMPORARY Hadamard computation
    def sethadamard(n):
        if n==1:
            return np.array([[1]], dtype=theano.config.floatX)
        else:
            H = sethadamard(n/2)
            col1 = np.concatenate((H, H), axis=0)
            col2 = np.concatenate((H, -H), axis=0)
            return np.sqrt(1./2) * np.concatenate((col1, col2), axis=1)
    Hadamard = sethadamard(n_hidden)
#    Hadamard = theano.shared(value=Hadamard_values, name='Hadamard')

    #index_permute = rng.permutation(n=n_hidden)
    index_permute = np.random.permutation(n_hidden)
 
    # TEMPORARY fix Permutation matrix
#    Permutation = T.zeros_like(Hadamard)
#    perm = numpy.random.permutation(n_hidden)
#    for i in xrange(n_hidden):
#        Permutation = T.set_subtensor(Permutation[perm[i], i], 1)                     

    # define the recurrence used by theano.scan
    def recurrence(x_t, h_t, theta, V_re, V_im, hidden_bias):    
        def times_diag(input, n_hidden, diag):
            input_re = input[:, :n_hidden]
            input_im = input[:, n_hidden:]
            Re = T.nlinalg.AllocDiag()(T.cos(diag))
            Im = T.nlinalg.AllocDiag()(T.sin(diag))
            input_re_times_Re = T.dot(input_re, Re)
            input_re_times_Im = T.dot(input_re, Im)
            input_im_times_Re = T.dot(input_im, Re)
            input_im_times_Im = T.dot(input_im, Im)

            return T.concatenate([input_re_times_Re - input_im_times_Im,
                                  input_re_times_Im + input_im_times_Re], axis=1)

        def vec_permutation(input, n_hidden, index_permute):
            re = input[:, :n_hidden]
            im = input[:, n_hidden:]
            re_permute = re[:, index_permute]
            im_permute = im[:, index_permute]

            return T.concatenate([re_permute, im_permute], axis=1)      
        

        # Compute hidden linear transform
        #import pdb; pdb.set_trace()
        step1 = times_diag(h_t, n_hidden, theta[0,:])
        step2 = T.concatenate([T.dot(step1[:, :n_hidden], Hadamard),
                               T.dot(step1[:, n_hidden:], Hadamard)], axis=1) 
        step3 = vec_permutation(step2, n_hidden, index_permute)
#        step3 = T.concatenate([T.dot(step2[:, :n_hidden], Permutation),
#                               T.dot(step2[:, n_hidden:], Permutation)], axis=1)
        step4 = times_diag(step3, n_hidden, theta[1,:])
        step5 = T.concatenate([T.dot(step4[:, :n_hidden], Hadamard),
                               T.dot(step4[:, n_hidden:], Hadamard)], axis=1)
        step6 = times_diag(step5, n_hidden, theta[2,:])
        
        hidden_lin_output = step6
        
        # Compute data linear transform
        data_lin_output_re = T.dot(x_t, V_re)
        data_lin_output_im = T.dot(x_t, V_im)
        data_lin_output = T.concatenate([data_lin_output_re, data_lin_output_im], axis=1)
        
        # Apply non-linearity 
        lin_output = hidden_lin_output + data_lin_output
        lin_output_re = lin_output[:, :n_hidden]
        lin_output_im = lin_output[:, n_hidden:] 

        modulus = T.sqrt(lin_output_re ** 2 + lin_output_im ** 2)
        scale = T.maximum(modulus + hidden_bias.dimshuffle('x',0), 0.) / (modulus + 1e-5)
        nonlin_output_re = lin_output_re * scale
        nonlin_output_im = lin_output_im * scale

        nonlin_output = T.concatenate([nonlin_output_re, 
                                       nonlin_output_im], axis=1)

        return nonlin_output

    h_0_batch = T.tile(h_0, [x.shape[1], 1])
    non_sequences = [theta, V_re, V_im, hidden_bias]
    hidden_states, updates = theano.scan(fn=recurrence,
                                         sequences=x,
                                         non_sequences=non_sequences,
                                         outputs_info=h_0_batch)

    for t in xrange(time_steps):
        RNN_output_t = T.dot(hidden_states[t,:,:], U) + out_bias.dimshuffle('x',0) 
        if t==0:
            cost = ((RNN_output_t - y[t,:,:])**2).mean() 
        else:
            cost += ((RNN_output_t - y[t,:,:])**2).mean()

    cost = cost / time_steps             
#    cost = ((lin_output - y)**2).mean()
    cost.name = 'MSE'

#    # define hidden to output graph
#    lin_output = T.dot(hidden_states[-1,:,:], U) + out_bias.dimshuffle('x', 0)
#    RNN_output = T.nnet.softmax(lin_output)

#    # define the cost
#    cost = T.nnet.categorical_crossentropy(RNN_output, y).mean()
#    cost.name = 'cross_entropy'

    return [x, y], parameters, cost, hidden_states

 
# Warning: assumes n_batch is a divisor of number of data points
# Warning: n_hidden must be a power of 2
def main(**kwargs):
    #print 'im in'

    n_input = 1
    n_hidden = 128
    n_output = 1
    n_batch = 10
    n_iter = 2000
    learning_rate = 0.02
    time_steps = 20
    gradient_clipping = 500
    momentum = 0.5

    index = T.iscalar('i')
    
    n_data = 1e4
    train_x = np.asarray(np.random.uniform(low=-1.,
                                           high=1.,
                                           size=(time_steps, n_data, 1)),
                         dtype=theano.config.floatX)

    train_y = np.cumsum(train_x, axis=0)
    num_batches = n_data / n_batch
    

    ##### MNIST processing ################################################
#    # load and preprocess the data
#    (train_x, train_y), (valid_x, valid_y), (test_x, test_y) = cPickle.load(gzip.open("mnist.pkl.gz", 'rb'))
#    n_data = train_x.shape[0]
#    num_batches = n_data / n_batch
#    # shuffle data order
#    inds = range(train_x.shape[0])
#    np.random.shuffle(inds)
#    train_x = np.ascontiguousarray(train_x[inds, :time_steps])
#    train_y = np.ascontiguousarray(train_y[inds])

    # reshape x
#    train_x = np.reshape(train_x.T, (time_steps, n_data, 1))
    
    # change y to one-hot encoding
#    temp = np.zeros((n_data, n_output)) 
#    temp[np.arange(n_data), train_y] = 1
#    train_y = temp.astype('float32')
    #######################################################################

    # compute the complex RNN theano graph and compute gradients
    inputs, parameters, cost, hidden_states = complex_RNN(n_input, n_hidden, n_output, time_steps)

    ##########
    #parameters = [V_re, V_im, hidden_bias, U, out_bias, theta, h_0]
    #parameters = [parameters[0], parameters[1], parameters[2]]
    ##########
    #import pdb; pdb.set_trace()
    gradients = T.grad(cost, parameters)

    s_train_x = theano.shared(train_x)
    s_train_y = theano.shared(train_y)

    # initialize rmsprop params 
    rmsprop = [theano.shared(1e-9*np.ones_like(p.get_value())) for p in parameters]
    new_rmsprop = [0.9 * vel + 0.1 * T.clip(g, -gradient_clipping, gradient_clipping)**2 
                   for vel, g in zip(rmsprop, gradients)]

    # compute updates
#    updates = zip(rmsprop, new_rmsprop)
#    for p, g, rms in zip(parameters, gradients, new_rmsprop):
#        clipped_g = T.clip(g, -gradient_clipping, gradient_clipping)
#        update = - learning_rate * clipped_g / T.sqrt(rms)
#        updates.append((p, p + update))

    # gradient descent with momentum updates and clipped gradients
    velocities = [theano.shared(np.zeros_like(p.get_value())) for p in parameters]
    updates = [(p, p + vel) for p, vel in zip(parameters, velocities)]
    updates = updates + [(vel, momentum * vel - learning_rate * T.clip(g, -gradient_clipping, gradient_clipping)) 
                         for vel, g in zip(velocities, gradients)]
    
    # gradient descent
#    updates = [(p, p - learning_rate * g) for p, g in zip(parameters, gradients)]
    

    # write training functions
    givens = {inputs[0] : s_train_x[:, n_batch * index : n_batch * (index + 1), :],
              inputs[1] : s_train_y[:, n_batch * index : n_batch * (index + 1), :]}

#    givens = {inputs[0] : s_train_x[:, n_batch * index : n_batch * (index + 1), :],
#              inputs[1] : s_train_y[n_batch * index : n_batch * (index + 1)]}

    givens_full = {inputs[0] : s_train_x,
                   inputs[1] : s_train_y}
    
    import pdb; pdb.set_trace()
    train = theano.function([index], cost, givens=givens, updates=updates)
    train_full = theano.function([], cost, givens=givens_full)

    for i in xrange(n_iter):
        train(i % num_batches)
        print train_full()



        if i==0:
            params0 = [p.get_value() for p in parameters]
        else:
            params1 = [p.get_value() for p in parameters] 
            steps = [(p1 - p0) for p0, p1 in zip(params0, params1)]
            max_steps = [np.amax(np.abs(step)) for step in steps]
       #     print steps
            params0 = params1
            print np.amax(max_steps)

if __name__=="__main__":
    main()
